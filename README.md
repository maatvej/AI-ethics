# README: Проект по обнаружению токсичных комментариев

## Описание проекта
Этот проект направлен на создание модели машинного обучения, которая определяет, является ли текстовый комментарий токсичным. Модель обучается на наборе данных с комментариями и оценками их токсичности, а затем используется для предсказания вероятности токсичности новых комментариев.

Проект также включает анализ этической стороны модели, особенно возможной предвзятости к упоминанию определенных религий или рас.

---

## Содержание

1. [Установка](#установка)
2. [Использование](#использование)
3. [Анализ результатов](#анализ-результатов)
4. [Выводы по этике ИИ](#выводы-по-этике-ии)
5. [Предложения по улучшению](#предложения-по-улучшению)

---

## Установка

### Требования:
- Python 3.x
- Библиотеки:
  - `numpy`
  - `pandas`
  - `scikit-learn`

### Установка зависимостей:
```bash
pip install numpy pandas scikit-learn
```

### Подготовка данных:
Данные должны быть представлены в формате CSV-файла (`data.csv`) со следующими колонками:
- `comment_text` — текст комментария.
- `target` — числовое значение от 0 до 1, представляющее уровень токсичности (чем ближе к 1, тем более токсичный комментарий).

---

## Использование

### Обучение модели:
1. Загрузите данные из файла `data.csv`.
2. Разделите данные на обучающую и тестовую выборки.
3. Преобразуйте текстовые комментарии в числовые признаки с помощью `CountVectorizer`.
4. Обучите модель логистической регрессии.
5. Оцените точность модели на тестовой выборке.

### Пример использования:
Функция `predict_toxicity(comment)` позволяет предсказать вероятность токсичности заданного комментария. Например:
```python
print(predict_toxicity("I love apples"))  # Вероятность токсичности около 0
print(predict_toxicity("Apples are stupid"))  # Вероятность токсичности около 1
```

---

## Анализ результатов

### Точность модели:
Модель достигает **accuracy = 0.928**, что указывает на хорошее качество классификации на тестовой выборке.

### Ключевые слова токсичности:
На основе коэффициентов модели были выявлены наиболее токсичные слова:
- `stupid`, `idiot`, `idiots`, `stupidity`, `idiotic`, `crap`, `dumb`, `pathetic`, `hypocrite`, `moron`.

Однако некоторые из этих слов могут встречаться в нейтральном контексте, поэтому важно учитывать семантику предложения целиком.

### Проверка на предвзятость:
Было проведено тестирование модели на наличие потенциальной предвзятости к упоминанию определенных групп:

| Комментарий                        | Вероятность токсичности |
|-----------------------------------|--------------------------|
| "I have a christian friend"       | 0.187                    |
| "I have a muslim friend"          | 0.511                    |
| "I have a white friend"           | 0.402                    |
| "I have a black friend"           | 0.589                    |

**Наблюдения:**
- Упоминания мусульман и чернокожих людей получили самую высокую оценку токсичности, несмотря на нейтральный характер комментариев.
- Это может свидетельствовать о предвзятости модели, вызванной особенностями обучающего набора данных.

---

## Выводы по этике ИИ

### Наличие предвзятости:
Модель демонстрирует различную чувствительность к упоминанию религий и рас, что может привести к этическим проблемам, таким как дискриминация и усиление стереотипов.

### Этично ли поведение модели?
Текущая реализация **не является полностью этичной**, так как:
- Может ошибочно помечать нейтральные комментарии как токсичные.
- Учет религиозных или расовых факторов может способствовать несправедливому отношению к пользователям.

---

## Предложения по улучшению

1. **Устранение предвзятости:**
   - Пересмотреть обучающий набор данных, исключив случаи систематического преувеличения токсичности для упоминаний определённых групп.
   - Использовать методы снижения предвзятости, такие как добавление шума или перераспределение весов.

2. **Улучшение качества модели:**
   - Использовать более сложные модели, такие как случайные леса или нейронные сети.
   - Добавить контекстуальные признаки, учитывающие эмоциональную окраску текста.

3. **Улучшение интерпретируемости:**
   - Визуализировать важные слова для конкретного комментария.
   - Добавить объяснения предсказаний модели для повышения прозрачности.

---
